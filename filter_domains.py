import csv
import requests
import time
import os
import json
import signal
from concurrent.futures import ThreadPoolExecutor, as_completed
from requests.adapters import HTTPAdapter, Retry
import random
import threading

# å…¨å±€åœæ­¢æ ‡å¿—
stop_all = False
cache_lock = threading.Lock()  # ç¼“å­˜å†™é”
file_lock = threading.Lock()   # æ–‡ä»¶å†™é”

def signal_handler(sig, frame):
    global stop_all
    print("\n[!] æ£€æµ‹åˆ° Ctrl+Cï¼Œæ­£åœ¨å®‰å…¨é€€å‡ºâ€¦â€¦")
    stop_all = True

signal.signal(signal.SIGINT, signal_handler)

BLACKLIST_KEYWORDS = [
    "sex", "porn", "casino", "bet", "gamble", "xxx", "escort",
    "poker", "adult", "lottery", "hentai", "baccarat"
]
# å–æ¶ˆåç¼€é™åˆ¶
# ACCEPTED_TLDS = ["com", "net", "org"]

MIN_BACKLINKS = 50
MIN_SNAPSHOTS = 5
MAX_RETRY = 3

def is_top_domain(info):
    return info['Backlinks'] > 3500 and info['Snapshots'] > 20 and info['ACR'] > 50.0

INPUT_FILE = "domains.csv"
OUTPUT_FILE = "filtered_domains.csv"
OUTPUT_TOP_FILE = "top_domains.csv"
OUTPUT_TEXT_FILE = "filtered_domains.txt"
FAILED_RETRY_FILE = "failed_retry_domains.json"
CACHE_FILE = "wayback_cache.json"

MAX_WORKERS = 10  # çº¿ç¨‹æ•°æ ¹æ®æœºå™¨è°ƒèŠ‚

PROXY_PORTS = range(30001, 30852)
PROXIES_LIST = [
    {
        "http": f"http://127.0.0.1:{port}",
        "https": f"http://127.0.0.1:{port}"
    }
    for port in PROXY_PORTS
]

HEADERS = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
                  "AppleWebKit/537.36 (KHTML, like Gecko) "
                  "Chrome/137.0.0.0 Safari/537.36"
}

session = requests.Session()
retries = Retry(total=3, backoff_factor=1, status_forcelist=[429, 500, 502, 503, 504])
session.mount('https://', HTTPAdapter(max_retries=retries))
session.headers.update(HEADERS)

def get_random_proxy():
    if PROXIES_LIST:
        return random.choice(PROXIES_LIST)
    else:
        return None

def is_safe_domain(domain):
    return not any(kw in domain.lower() for kw in BLACKLIST_KEYWORDS)

def load_cache(filename):
    if os.path.exists(filename):
        with open(filename, 'r', encoding='utf-8') as f:
            return json.load(f)
    return {}

def save_cache(cache, filename):
    with cache_lock:
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(cache, f, ensure_ascii=False, indent=2)

def load_passed_domains():
    passed = set()
    if os.path.exists(OUTPUT_FILE):
        with open(OUTPUT_FILE, 'r', encoding='utf-8') as f:
            next(f)  # è·³è¿‡è¡¨å¤´
            for line in f:
                domain = line.split(',')[0].strip().lower()
                passed.add(domain)
    return passed

def load_failed_retry_domains():
    if os.path.exists(FAILED_RETRY_FILE):
        with open(FAILED_RETRY_FILE, 'r', encoding='utf-8') as f:
            return json.load(f)
    return {}

def save_failed_retry_domains(data):
    with open(FAILED_RETRY_FILE, 'w', encoding='utf-8') as f:
        json.dump(data, f, ensure_ascii=False, indent=2)

# åé“¾æ•°è§£æå‡½æ•°ï¼Œæ”¯æŒå¸¦å•ä½å¦‚ "29.9 K", "1.2M"
def parse_backlinks(value):
    if not value:
        return 0
    value = str(value).strip().lower().replace(',', '')
    multiplier = 1
    if value.endswith('k'):
        multiplier = 1_000
        value = value[:-1]
    elif value.endswith('m'):
        multiplier = 1_000_000
        value = value[:-1]
    elif value.endswith('b'):
        multiplier = 1_000_000_000
        value = value[:-1]
    try:
        return int(float(value) * multiplier)
    except:
        return 0

# Waybackè·å–å¿«ç…§æ•°é‡
def query_wayback(domain, timeout=5):
    url = f"https://web.archive.org/cdx/search/cdx?url={domain}&output=json"
    try:
        resp = session.get(url, proxies=get_random_proxy(), timeout=timeout)
        if resp.status_code == 200:
            data = resp.json()
            return max(len(data) - 1, 0)  # ç¬¬ä¸€è¡Œä¸ºå­—æ®µå
    except Exception as e:
        print(f"[Wayback] å¿«ç…§è¯·æ±‚é”™è¯¯ {domain} -> {e}")
    return None

def query_memento(domain, timeout=5):
    url = f"http://timetravel.mementoweb.org/api/json/{domain}"
    try:
        resp = session.get(url, proxies=get_random_proxy(), timeout=timeout)
        if resp.status_code == 200:
            data = resp.json()
            if 'mementos' in data and 'list' in data['mementos']:
                return len(data['mementos']['list'])
    except Exception as e:
        print(f"[Memento] å¿«ç…§è¯·æ±‚é”™è¯¯ {domain} -> {e}")
    return None

def query_archive_today(domain, timeout=5):
    url = f"https://archive.ph/{domain}"
    try:
        resp = session.get(url, proxies=get_random_proxy(), timeout=timeout)
        if resp.status_code == 200 and "Wayback Machine" not in resp.text:
            return 1
    except Exception as e:
        print(f"[Archive.today] å¿«ç…§è¯·æ±‚é”™è¯¯ {domain} -> {e}")
    return None

def get_wayback_snapshots(domain):
    for func in [query_wayback, query_memento, query_archive_today]:
        count = func(domain)
        if count is not None:
            return count
    return -1

# æ–°å¢ï¼šåˆ¤æ–­æ˜¯å¦ä¸€çº§åŸŸåï¼Œæ’é™¤å¤šçº§
def is_valid_domain(domain):
    domain = domain.lower().strip()
    if domain.count('.') != 1:
        return False, "éä¸€çº§åŸŸå"
    return True, None

def process_domain(row, cache, passed_domains):
    global stop_all
    if stop_all:
        return None, "ç”¨æˆ·è¯·æ±‚ç»ˆæ­¢"

    domain = row.get('domain') or row.get('Domain')
    if not domain:
        return None, "åŸŸåç¼ºå¤±"

    domain_lc = domain.lower()

    # åˆ¤æ–­æ˜¯å¦ä¸€çº§åŸŸå
    valid, reason = is_valid_domain(domain_lc)
    if not valid:
        return domain_lc, reason

    if domain_lc in passed_domains:
        return domain_lc, "ä¹‹å‰å·²åˆæ ¼ï¼Œè·³è¿‡"

    # ä¸å†åˆ¤æ–­åç¼€æ˜¯å¦æ¥å—ï¼Œå·²åœ¨ is_valid_domain åˆ¤æ–­è¿‡

    # åé“¾æ•°ç”¨æ–°çš„è§£æå‡½æ•°
    backlinks_raw = row.get('backlinks') or row.get('Backlinks') or row.get('bl') or 0
    backlinks = parse_backlinks(backlinks_raw)

    try:
        acr = float(row.get('acr') or row.get('ACR') or 0)
    except:
        acr = 0.0

    try:
        wby = int(row.get('wby') or row.get('WBY') or 0)
    except:
        wby = 0

    try:
        aby = int(row.get('aby') or row.get('ABY') or 0)
    except:
        aby = 0

    if not is_safe_domain(domain):
        return domain_lc, "åŒ…å«æ•æ„Ÿè¯"
    if backlinks < MIN_BACKLINKS:
        return domain_lc, f"åé“¾æ•°ä½({backlinks}<{MIN_BACKLINKS})"

    with cache_lock:
        snapshots = cache.get(domain_lc)
    if snapshots is None:
        snapshots = get_wayback_snapshots(domain_lc)
        with cache_lock:
            cache[domain_lc] = snapshots

    if snapshots == -1:
        return domain_lc, "å¿«ç…§è¯·æ±‚å¤±è´¥"
    if snapshots < MIN_SNAPSHOTS:
        return domain_lc, f"å¿«ç…§æ•°ä½({snapshots}<{MIN_SNAPSHOTS})"

    info = {
        'Domain': domain_lc,
        'Backlinks': backlinks,
        'Snapshots': snapshots,
        'ACR': acr,
        'WBY': wby,
        'ABY': aby,
        'Top': is_top_domain({
            'Backlinks': backlinks,
            'Snapshots': snapshots,
            'ACR': acr
        })
    }
    return info, None

def extract_top_from_filtered():
    if not os.path.exists(OUTPUT_FILE):
        print("âŒ æ‰¾ä¸åˆ°å·²ç­›é€‰æ–‡ä»¶ã€‚")
        return

    TOP_TEXT_FILE = "top_domains_sorted.txt"
    domains = []

    with open(OUTPUT_FILE, 'r', encoding='utf-8') as infile:
        reader = csv.DictReader(infile)
        for row in reader:
            try:
                backlinks = int(row['Backlinks'])
                snapshots = int(row['Snapshots'])
                acr = float(row['ACR'])

                if is_top_domain({'Backlinks': backlinks, 'Snapshots': snapshots, 'ACR': acr}):
                    domains.append({
                        'Domain': row['Domain'],
                        'Backlinks': backlinks,
                        'Snapshots': snapshots,
                        'ACR': acr
                    })
            except Exception:
                continue

    # è‡ªå®šä¹‰æ’åºæƒé‡ï¼ˆä½ å¯ä»¥æ ¹æ®éœ€æ±‚è°ƒæ•´æƒé‡æ¯”ä¾‹ï¼‰
    def score(d):
        return d['Backlinks'] * 0.5 + d['Snapshots'] * 0.3 + d['ACR'] * 0.2

    # æŒ‰ç»¼åˆå¾—åˆ†é™åºæ’åˆ—
    domains.sort(key=score, reverse=True)

    with open(TOP_TEXT_FILE, 'w', encoding='utf-8') as outfile:
        for d in domains:
            line = f"{d['Domain']} | åé“¾: {d['Backlinks']} | å¿«ç…§: {d['Snapshots']} | ACR: {d['ACR']}\n"
            outfile.write(line)

    print(f"âœ… å…±æå–æå“åŸŸå {len(domains)} æ¡ï¼Œå·²æŒ‰ç»¼åˆè¯„åˆ†æ’åºå¹¶ä¿å­˜åˆ° {TOP_TEXT_FILE}")

def filter_domains():
    global stop_all

    if not os.path.exists(INPUT_FILE):
        print(f"âŒ æœªæ‰¾åˆ°è¾“å…¥æ–‡ä»¶ï¼š{INPUT_FILE}")
        return

    passed_domains = load_passed_domains()
    failed_retry_domains = load_failed_retry_domains()
    snapshot_cache = load_cache(CACHE_FILE)

    input_domains = {}
    with open(INPUT_FILE, newline='', encoding='utf-8') as infile:
        reader = csv.DictReader(infile)
        for row in reader:
            domain = (row.get('domain') or row.get('Domain') or '').lower()
            if not domain or domain in passed_domains or domain in failed_retry_domains:
                continue
            input_domains[domain] = {"row": row, "retry_count": 0}

    # åˆå¹¶å¤±è´¥é‡è¯•åˆ—è¡¨
    for domain, data in failed_retry_domains.items():
        input_domains[domain] = data

    with open(OUTPUT_FILE, 'a', newline='', encoding='utf-8') as outfile, \
         open(OUTPUT_TEXT_FILE, 'a', encoding='utf-8') as textfile:

        fieldnames = ['Domain', 'Backlinks', 'Snapshots', 'ACR', 'WBY', 'ABY']
        writer = csv.DictWriter(outfile, fieldnames=fieldnames)
        if os.stat(OUTPUT_FILE).st_size == 0:
            writer.writeheader()

        failed_next_retry = {}

        with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
            futures = {}
            for domain, data in input_domains.items():
                futures[executor.submit(process_domain, data['row'], snapshot_cache, passed_domains)] = (domain, data.get('retry_count', 0))

            try:
                for future in as_completed(futures):
                    if stop_all:
                        print("[!] ç”¨æˆ·è¯·æ±‚ç»ˆæ­¢ï¼Œåœæ­¢å¤„ç†å‰©ä½™ä»»åŠ¡ã€‚")
                        break

                    domain, retry_count = futures.pop(future)
                    try:
                        info, error = future.result()
                    except Exception as e:
                        print(f"[!] ä»»åŠ¡å¼‚å¸¸: {domain} -> {e}")
                        if retry_count + 1 < MAX_RETRY:
                            failed_next_retry[domain] = {"row": input_domains[domain]['row'], "retry_count": retry_count + 1}
                        else:
                            print(f"[!] åŸŸå {domain} è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°ï¼Œæ”¾å¼ƒã€‚")
                        continue

                    if error:
                        if error == "ä¹‹å‰å·²åˆæ ¼ï¼Œè·³è¿‡":
                            print(f"â­ è·³è¿‡: {domain} | {error}")
                            failed_next_retry.pop(domain, None)
                        elif error == "ç”¨æˆ·è¯·æ±‚ç»ˆæ­¢":
                            print(f"[!] ç”¨æˆ·è¯·æ±‚ç»ˆæ­¢ï¼Œè·³è¿‡åç»­ã€‚")
                            stop_all = True
                            break
                        elif error == "å¿«ç…§è¯·æ±‚å¤±è´¥":
                            if retry_count + 1 < MAX_RETRY:
                                print(f"ğŸŸ¡ ä¸åˆæ ¼: {domain} | {error}ï¼Œæ­£åœ¨ç¬¬ {retry_count+1} æ¬¡é‡æ–°è¯·æ±‚â€¦â€¦")
                                failed_next_retry[domain] = {"row": input_domains[domain]['row'], "retry_count": retry_count + 1}
                            else:
                                print(f"âŒ ä¸åˆæ ¼: {domain} | åŸå› : è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°ï¼Œæ”¾å¼ƒ")
                        else:
                            print(f"âŒ ä¸åˆæ ¼: {domain} | åŸå› : {error}")
                    else:
                        with file_lock:
                            print(f"âœ… åˆæ ¼: {domain} | åé“¾: {info['Backlinks']} | å¿«ç…§: {info['Snapshots']}")
                            writer.writerow({k: info[k] for k in fieldnames})
                            textfile.write(f"{domain} | åé“¾: {info['Backlinks']} | å¿«ç…§: {info['Snapshots']} | ACR: {info['ACR']}\n")
                        failed_next_retry.pop(domain, None)

                    save_cache(snapshot_cache, CACHE_FILE)

            except KeyboardInterrupt:
                stop_all = True
                print("\n[!] æ•è·åˆ°ä¸­æ–­ï¼Œå®‰å…¨é€€å‡ºã€‚")

        save_cache(snapshot_cache, CACHE_FILE)
        save_failed_retry_domains(failed_next_retry)

    print("\nâœ… ç­›é€‰å®Œæˆï¼åˆæ ¼åŸŸåå·²ä¿å­˜åˆ°", OUTPUT_FILE)
    if failed_next_retry:
        print(f"âš ï¸ å¤±è´¥çš„åŸŸåå·²ä¿å­˜åˆ° {FAILED_RETRY_FILE}ï¼Œä¸‹æ¬¡è¿è¡Œæ—¶ä¼šè‡ªåŠ¨é‡è¯•ã€‚")
    else:
        print("ğŸ‰ æ‰€æœ‰åŸŸåå¤„ç†å®Œæˆï¼Œæ— éœ€é‡è¯•ã€‚")

if __name__ == "__main__":
    print("è¯·é€‰æ‹©æ¨¡å¼ï¼š\n1 - ç­›é€‰åŸŸåï¼ˆä»åŸå§‹ CSV ä¸­è¯»å–ï¼Œè¾“å‡ºç­›é€‰ç»“æœï¼‰\n2 - ä»å·²ç­›é€‰åŸŸåä¸­æå–æå“")
    mode = input("è¾“å…¥é€‰é¡¹ (1/2)ï¼š").strip()
    if mode == "1":
        filter_domains()
    elif mode == "2":
        extract_top_from_filtered()
    else:
        print("æ— æ•ˆé€‰é¡¹ï¼Œé€€å‡ºã€‚")
